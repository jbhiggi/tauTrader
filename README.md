# A Categorical LSTM to Predict Stock Price Movements  

### Turning noisy market data into actionable signals with LSTM-based threshold classification.

![Python](https://img.shields.io/badge/python-3.10-blue.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)

Classical models such as **Blackâ€“Scholes** assume stock log-returns follow a Brownian motion. While this approximation holds in the long run, real markets exhibit **short-term autocorrelations and structural deviations** from pure randomness. These pockets of predictability are noisy but exploitable, creating opportunities for systematic strategies.  

Instead of forecasting exact return magnitudes (a regression task prone to error in noisy data), this project reframes the problem as a **categorical classification**:  
- **Up** if price increases beyond a positive threshold  
- **Down** if price decreases beyond a negative threshold  
- **Neutral** if price remains within the threshold band  

This shift focuses the model on directional movement rather than precise values, making predictions more robust and directly actionable.  

We apply **LSTM neural networks**, which are well suited to capture temporal dependencies in sequential financial data. Compared to rigid rules-based approaches, LSTMs offer **greater flexibility and adaptability across assets, horizons, and regimes**. Even modest accuracy above random chance (50%) can translate into **profitable trading signals** when paired with sound risk management.  

This repository demonstrates how **modern sequence models can extract signal from market noise** and provides a reproducible framework for threshold-based prediction, evaluation, and deployment.  

### Conceptual Illustration

| ![Fig 1](docs/label_one_dummy_figure.png) | ![Fig 2](docs/label_zero_dummy_figure.png) |
|-------------------------------------------|--------------------------------------------|
| **Class label `1` example:** Dummy data illustrating when the stockâ€™s closing price exits the box **above the defined threshold**. In this setup, class `1` can alternatively represent cases where the closing price exits **below the box**, depending on the labeling convention. | **Class label `0` example:** Dummy data illustrating when the stockâ€™s closing price exits the box **without crossing above the threshold**. If the task is to predict upward movements, closing prices that exit **below the box** are also labeled as `0`. |


## ğŸ“‹ Quick Navigation

| Section              | Link                                                         |
| -------------------- | ------------------------------------------------------------ |
| ğŸš€ Executive Summary | [Jump to Executive Summary](#executive-summary)              |
| ğŸ¯ Motivation        | [Jump to Motivation](#motivation)                            |
| ğŸ”¬ Methodology       | [Jump to Methodology](#methodology)                          |
| ğŸ“Š Experiments       | [Jump to Experiments](#experiments)                          |
| ğŸ–¼ï¸ Results          | [Jump to Results](#results)                                  |
| âš™ï¸ How to Reproduce  | [Jump to How-to-Reproduce](#how-to-reproduce)                |
| ğŸ“˜ User Guide        | [Jump to User Guide](#user-guide)                            |
| ğŸš§ Limitations       | [Jump to Limitations & Next Steps](#limitations--next-steps) |
| ğŸ“‚ Directory         | [Jump to Directory Structure](#directory-structure)          |
| ğŸ“š Further Docs      | [Jump to Further Documentation](#further-documentation)      |

---

## Executive Summary

This project develops a **binary classifier that predicts stock price movements** using LSTM sequence models, delivering a practical tool for anticipating upward vs. non-upward trends.  
A key methodological contribution is a **k-fold cross-validation mapping** that links validation-optimal thresholds (Ï„_val) to expected test-optimal thresholds (Ï„_test), ensuring more reliable deployment.  
Together, these components provide:

- A reproducible training and evaluation pipeline.
- Transparent performance metrics and diagnostics.
- Deployment-ready threshold mapping that reflects real-world conditions.
- Demonstrated, substantial improvement over baseline performance.

---

## Motivation

Accurately predicting **stock price movements** is a core challenge in quantitative finance, where even small improvements in precision can drive meaningful returns.  
While powerful sequence models such as LSTMs can capture temporal patterns, **their raw outputs still depend on the choice of decision threshold**.  
A threshold tuned only on validation data often fails to generalize, leading to **over-optimistic backtests and degraded live performance**.

To address this, we introduce a **cross-validationâ€“based mapping** from Ï„_val â†’ Ï„_test:  
- Empirically corrects for the validationâ€“test mismatch.  
- Can be applied in deployment without access to test labels.  

The result is a system that combines a **strong predictive model** with **robust threshold calibration**, producing more reliable and stable trading signals while reducing the risk of model decay in production.

---

## Methodology

ğŸ“Š **Flow Diagram**  

```
Data â†’ Feature Engineering â†’ Normalization â†’ Sequence Creation & Splitting
    â†’ LSTM Training â†’ Threshold Optimization â†’ K-Fold CV
    â†’ Linear Mapping (Ï„_val â†’ Ï„_test) â†’ Deployment
```

Our approach follows a structured, config-driven end-to-end pipeline designed for reproducible time-series classification and robust threshold selection. The methodology balances statistical rigor with practical deployment considerations:

1. **Data Preprocessing & Normalization**  
   Raw OHLC price data is augmented with technical indicators (`SMA`, `MACD`, `Bollinger Bands`, `Stochastic Oscillator`, `% Returns`), clipped for outliers, and normalized using leak-safe scaling strategies, with accompanying [diagnostic plots](results/APPL_daily_experiment/figures/APPL_daily_experiment_feature_norm_comparison_train.pdf) to verify normalization quality.

2. **Sequence Creation & Data Splitting**  
   Overlapping rolling windows are constructed to capture temporal dependencies. Splits are time-ordered to respect causality.  

3. **Model Training**  
   A **regularized LSTM classifier** is trained to capture non-linear temporal dependencies in multivariate price series.  
   - **Robustness:** Class weighting mitigates imbalance between directional regimes (up, down, flat).  
   - **Generalization:** Early stopping with validation monitoring guards against overfitting to market noise.  
   - **Optimization:** The Adam optimizer with carefully tuned learning rates ensures stable convergence in non-stationary settings.  
   - **Statistical rigor:** Performance is tracked using both aggregate accuracy and class-level diagnostics (precision, recall), enabling detection of asymmetric error profiles.  

   This setup emphasizes **signal extraction under heavy noise**, balancing flexibility with regularization â€” a practical approach for financial time series where sample sizes are limited and overfitting risk is high.

4. **Threshold Optimization**  
   Decision thresholds are tuned against precision, recall, and F1 to identify robust operating points.  

5. **K-Fold Cross-Validation**  
   Multiple folds are used to characterize the relationship between validation and test thresholds, reducing variance in model selection.  

6. **Threshold Mapping for Deployment**  
   A linear relation Ï„_test â‰ˆ aÂ·Ï„_val + b, derived from k-fold analysis, provides a principled way to estimate test-time thresholds from validation results, an essential step for real world deployment of models.


ğŸ”— **Detailed Documentation**  
For full explanations of each stage, see:  
- [docs/methodology.md](docs/methodology.md) â€“ complete methodology document  
- [docs/kfold_cv.md](docs/kfold_cv.md) â€“ in-depth k-fold cross-validation analysis  

---

## Experiments

* Dataset: **AAPL 8-year daily data**  
* Model: LSTM binary classifier  
* Objective metric: **Precision** (default), with support for Recall, F1  
* Baselines:  
  - **Random guessing** â€” establishes whether the model captures meaningful signal.  
  - **NaÃ¯ve Ï„_val threshold** â€” compared against the adjusted Ï„ from the CV mapping procedure.
---

## Results

### Our model alone beats random (+1.98%), and our pipeline mapping elevates test precision to **+15.7% over baseline**.


### 1. **Performance Evidence**

- **Training & Model Selection**  
  - We generate a series of [diagnostic plots](results/APPL_daily_experiment/figures/APPL_daily_experiment_combined_diagnostics.pdf) to monitor **training dynamics** (loss evolution, validation accuracy, learning rate) and **decision behavior** (threshold probability distributions). These diagnostics confirm stability, convergence, and the interpretability of the modelâ€™s outputs.
  - The model reached its **best validation loss at epoch 14**, and this version was used for evaluation.  

- **Validation Performance**  
  - Default threshold Ï„ = 0.5: **Precision = 64.8%**, above the random baseline of 61.8%.  
  - After optimization, the **chosen Ï„_val = 0.530** achieved **Precision = 67.5%** with **Recall = 87.0%**.
  - This equates to a perfomance on the test set of **+1.98% over baseline** (See Figure 3).


<!-- First row: Confusion matrix centered -->
<p align="center">
  <img src="master_figures/APPL_daily_experiment_val_confusion_matrix.png" 
       alt="Confusion Matrix" width="50%">
</p>
<p align="center"><sub><i>Figure 1. <b>Confusion Matrix</b> â€“ Raw classification outcomes (TP, FP, TN, FN) for the validation set. The numbers are calculated from the default threshold Ï„_val = 0.5.</i></sub></p>

<!-- Second row: PR curves side by side -->
<p align="center">
  <img src="master_figures/APPL_daily_experiment_pr_curve_val_tau_mapping.png" 
       alt="Validation PR Curve" width="55%">
  <img src="master_figures/APPL_daily_experiment_pr_curve_test_tau_naive.png" 
       alt="Test PR Curve Naive" width="55%">
</p>
<p align="center"><sub><i>Figure 2. <b>Validation Precisionâ€“Recall Curve (before mapping)</b> â€“ The color of the curve encodes the classification threshold used at each point. The dashed red line indicates the random baseline (precision = 0.62). The red point represents the chosen optimal threshold.  
<br>Figure 3. <b>Test Precisionâ€“Recall Curve (before mapping)</b> â€“ The color of the curve encodes the classification threshold used at each point. The dashed red line indicates the random baseline (precision = 0.55). The red point represents the same (naÃ¯ve) threshold from the validation PR curve.</i></sub></p>


### 2. **K-Fold CV Threshold Mapping** 

- **Cross-Validation Diagnostics**  
  - [Plots](results/APPL_daily_experiment/figures/APPL_daily_experiment_k_fold_diagnostics.pdf) show the Ï„_val â†’ Ï„_test linear fit and fold-by-fold performance stability.

- **Cross-Validation Threshold Mapping**  
  - Linear fit coefficients: **a = -0.1018, b = 0.6086** (See Figure 4).  
  - Ï„_val = 0.530 mapped to **Ï„_test = 0.555**.  

- **Test Performance (out-of-sample)**  
  - At Ï„_test = 0.555: **Precision = 70.3%**, **Recall = 18.1%**.  
  - This represents a **+15.7% improvement over the baseline (54.5%)**.  
  - High precision at lower recall highlights a conservative decision boundaryâ€”exactly the property desired for deployment scenarios.  

<!-- First row: Threshold mapping centered -->
<p align="center">
  <img src="master_figures/APPL_daily_experiment_val_to_test_fit.png" 
       alt="Validation-to-Test Threshold Mapping" width="55%">
</p>
<p align="center"><sub><i>Figure 4. <b>Validation-to-Test Threshold Mapping</b> â€“ Pairs of optimal thresholds from validation and test folds (blue points) with a fitted linear model (red dashed line) and Â±1Ïƒ error band (shaded region).</i></sub></p>

<!-- Second row: PR curves side by side -->
<p align="center">
  <img src="master_figures/APPL_daily_experiment_pr_curve_val_tau_mapping.png" 
       alt="Validation PR Curve with Optimal Ï„" width="55%">
  <img src="master_figures/APPL_daily_experiment_pr_curve_test_tau_mapping.png" 
       alt="Test PR Curve with Optimal Ï„" width="55%">
</p>
<p align="center"><sub><i>Figure 5. <b>Validation PR Curve with Optimal Ï„</b> â€“ Validation set PR curve with the point chosen for k-fold derivation marked.  
<br>Figure 6. <b>Test PR Curve with Optimal Ï„</b> â€“ Test set PR curve with the k-foldâ€“derived optimal threshold marked.</i></sub></p>


### 3. Implications for a Trading Strategy

The model is learning meaningful structure in the data; the improvement in precision validates that the workflow can deliver **non-trivial predictive signals** for financial markets. Looking forward, precision is just one lens. In trading, **profitability depends not only on being right, but on how much you gain when right versus how much you lose when wrong**. For this reason, two more **directly actionable objectives** are natural extensions:  

- **Sharpe Ratio (SR):** Balances expected return against risk/volatility, reflecting the quality of a strategyâ€™s risk-adjusted performance.  
- **Expected Value (EV) per trade:** Captures the net profitability by weighting win sizes and loss sizes against their probabilities.  

Optimizing over SR or EV moves the research closer to a deployable trading strategy, since these metrics directly incorporate **risk and payoff structure**, not just classification accuracy.

More concretely, the **expected value (EV) per trade** at threshold Ï„ is defined as:

EV(Ï„) = p(Ï„) * Î¼_win(Ï„) + (1 â€“ p(Ï„)) * Î¼_loss(Ï„) â€“ C

where:  
- p(Ï„) = probability of a correct prediction at threshold Ï„ (model precision for trades taken)  
- Î¼_win(Ï„) = average return per winning trade, conditional on being correct  
- Î¼_loss(Ï„) = average return per losing trade, conditional on being wrong (typically negative)  
- C = average round-trip transaction costs (commission, slippage, borrow fees)

In practice, our K-fold cross-validation mapping can be extended to run a full backtest that computes EV across different values of Ï„. This allows us to **select the threshold that maximizes expected profitability**, making the output directly actionable as a trading strategy.

---

## How to Reproduce

```bash
git clone https://github.com/jbhiggi/tauTrader
cd tauTrader 
pip install -e .

# Run all experiments
python run_all.py
```

---

## Limitations & Next Steps  

This project is a **proof of concept** rather than a production trading system. Results are constrained by the small dataset (~2,000 daily candles) and non-optimized hyperparameters. A dedicated **Hyperparameter Optimization Pipeline (HOP)** is in progress and expected to yield performance gains. Future improvements include training on larger and higher-frequency data, refining the labeling strategy, and integrating the model with a buy/sell signal generator. Evaluation should also shift from pure classification metrics toward finance-specific objectives such as Sharpe ratio or expected value.  

See [Limitations](docs/limitations.md) for full details.

---

## Directory Structure

```bash
.
â”œâ”€â”€ README.md # Project overview
â”œâ”€â”€ LICENSE # License information
â”œâ”€â”€ pyproject.toml # Package configuration
â”œâ”€â”€ run_all.py # Script to run the full pipeline
â”‚
â”œâ”€â”€ docs/ # Documentation and supporting figures
â”‚  â”œâ”€â”€ kfold_cv.md
â”‚  â”œâ”€â”€ model_training.md
â”‚  â”œâ”€â”€ limitations.md
â”‚  â””â”€â”€ *.png
â”‚
â”œâ”€â”€ experiments/ # Notebooks & scripts for exploratory runs
â”‚  â”œâ”€â”€ train_model.py
â”‚  â”œâ”€â”€ train_kfold.py
â”‚  â””â”€â”€ map_tau.py
â”‚
â”œâ”€â”€ quant_lstm/ # Core package code
â”‚  â”œâ”€â”€ __init__.py
â”‚  â”œâ”€â”€ data_loader.py
â”‚  â”œâ”€â”€ preprocessing.py
â”‚  â”œâ”€â”€ feature_engineering.py
â”‚  â”œâ”€â”€ model.py
â”‚  â”œâ”€â”€ train.py
â”‚  â”œâ”€â”€ evaluate.py
â”‚  â”œâ”€â”€ cv_evaluate.py
â”‚  â”œâ”€â”€ map_tau_utils.py
â”‚  â””â”€â”€ configs/
â”‚     â””â”€â”€ config.yaml # Central configuration file
â”‚
â”œâ”€â”€ raw_data/ # Example dataset(s)
â”‚  â””â”€â”€ apple_data.csv
â”‚
â”œâ”€â”€ results/ # Saved model outputs & reports
â”‚  â””â”€â”€ APPL_daily_experiment/
â”‚     â”œâ”€â”€ figures/ # Diagnostic & PR curve plots
â”‚     â”œâ”€â”€ modified_data/ # Normalized and clipped datasets
â”‚     â”œâ”€â”€ *.json # Training reports, linear fits, metadata
â”‚     â””â”€â”€ *.keras # Saved model weights
â”‚
â””â”€â”€ master_figures/ # Publication-quality figures
â””â”€â”€ *.png
```

---

## Further Documentation

| ğŸ“Š Topic                   | Link                                   |
|----------------------------|----------------------------------------|
| ğŸ” K-Fold Cross-Validation | [K-fold CV](docs/kfold_cv.md)          |
| ğŸ“‘ Methodology Doc         | [Full Methodology](docs/methodology.md)|
| âš ï¸ Limitations             | [Limitations](docs/limitations.md)     |

---

## License  
MIT License. See [LICENSE](LICENSE) for details.